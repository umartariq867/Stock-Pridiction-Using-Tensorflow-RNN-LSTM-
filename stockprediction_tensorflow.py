# -*- coding: utf-8 -*-
"""StockPrediction Tensorflow.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13abRcqGNvnVhePoNU8vJDGn5kQgeaON2
"""

# Test if Google cloud is ready for running.
print('Dan Lo')

# The first time to run the program, you need to install yfinance module and 
# yahoo_earings_calendar by running the folloiwng command once.
# After that, you can comment them out by prefixing a # sign. 
!pip install yfinance
!pip install yahoo_earnings_calendar

# Import required library modules
import yfinance as yf
from yahoo_earnings_calendar import YahooEarningsCalendar
import pandas as pd
import numpy as np

# Try to get tickers from earnings report
# You may enter your prefered stock ticker manually.
yec= YahooEarningsCalendar()

from datetime import datetime
day1 = datetime.strptime('10/30/2020', '%m/%d/%Y')
er = yec.earnings_on(day1)

# Use this dropdown widget to select a ticker for retrieving the stock data.
from ipywidgets import Dropdown
option_list =[]
for i in range(len(er)):
  option_list.append([er[i]['companyshortname']+'-'+er[i]['ticker'], er[i]['ticker']])
dropdown = Dropdown(description = 'Select one company', options = option_list)
display(dropdown)
ticker = dropdown.value

# Get the stock data from Yahoo finance.
# If your stock sticker is not found in the dropdown menu, just enter it here to replace 'ticker' variable.
data = yf.download(ticker,'2019-01-01','2020-05-22')

# Take a look at the data
data.head

# Check the column headings
# We will use Adj Close for our training data.
data.columns

# Slice out the Adj Close column for our study.
# Take a look at the data.
y = data['Adj Close']
y.index=data.index
y.plot()

# Convert Pandas series to numpy ndarray for further processing.
y = y.values

# Split data into trainning and testing using 80/20 ratio.
training_size = int(y.size * 0.8)
print(training_size)

# Data normalization using training data's mean and standard deviation.
# y is a ndarray now and its dimension is 1.
training_mean = y[:training_size].mean()
training_std = y[:training_size].std()
# standarize the data
y = (y-training_mean)/training_std

# Take a look at the training data
y[0:5]

# Define a function to prepare data.
# The time series data are reorganized to form feature vectors and target values.
# For example, let sHistory be 10. A datapoint as a target will have 10 immediately 
# previous 10 values in the time series for its feature vector. 
def data_preprocess(dataset, iStart, iEnd, sHistory):
  data = []
  target = []
  # begin with sHistory to have historical data
  iStart += sHistory
  if iEnd is None:
    iEnd = len(dataset)
  for i in range(iStart, iEnd):
    indices = range(i-sHistory, i)
    data.append(np.reshape(dataset[indices], (sHistory, 1)))
    target.append(dataset[i])
  return np.array(data), np.array(target)

# Let's define number of history data for training and split data into training nd testing datasets.
past_history = 10
xtrain, ytrain = data_preprocess(y, 0, training_size, past_history)
xtest, ytest = data_preprocess(y, training_size, None, past_history)

# Take a look at data.
xtest[0]

# Take a look at data.
xtest[1]

# It's time to get Tensorflow library in and create an LSTM model.
import tensorflow as tf
# Setting seed to ensure reproducibility.
tf.random.set_seed(8888)
model = tf.keras.models.Sequential([
  tf.keras.layers.LSTM(8, input_shape=xtrain.shape[-2:]),
  tf.keras.layers.Dense(1)
])

# Take a look at the model you have just created.
model.summary()

# Before taining, you need to configure the model for training.
model.compile(optimizer='Adam',
              loss='mse')

BATCH_SIZE = 16
BUFFER_SIZE = 10000

p_train = tf.data.Dataset.from_tensor_slices((xtrain, ytrain))
p_train = p_train.cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE).repeat()

p_test = tf.data.Dataset.from_tensor_slices((xtest, ytest))
p_test = p_test.batch(BATCH_SIZE).repeat()

# For your model to be able to predict something, your need to train your model using training data.
history = model.fit(p_train, epochs = 10, steps_per_epoch=200, validation_data=p_test, validation_steps=50)

# Convert training history to datafram for viewing and graphing.
hist = pd.DataFrame(history.history)
hist.head()

# Training errors with respect to ecochs.
# loss: values of the cost function for training data.
# val_loss: values of the cost function for cross-validation data.
hist.plot()

# Now, we can predict stock values after model is trained. 
# We keep the predicted values and target vlaues for graphics later.
# You may run on p_train or p_test to check the preformance.
predicted_value = []
y_orig = []
count =0;
for x, y in p_train.take(20):
  y_orig.append(y[0].numpy())
  t_pred= model.predict(x)
  predicted_value.append(t_pred[0])

# Plot the predicted value and the target values together.
y_pred = [predicted_value[i][0] for i in range(len(predicted_value))]
result = pd.DataFrame({'predicted':y_pred, 'target':y_orig})
result.plot()